{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import json\n",
    "import h5py\n",
    "from utils import img_data_2_mini_batch, imgs2batch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_f = 'cocoqa_data_prepro_'\n",
    "base_n = '5000'\n",
    "base_fn = base_f + base_n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_h5 = h5py.File(base_fn+'.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_data_json = json.load(open(base_fn+'.json', 'r'))\n",
    "# pad fix\n",
    "itow = val_data_json['ix_to_word']\n",
    "\n",
    "wtoi = {iv: ik for ik,iv in itow.items()}\n",
    "old_pad = wtoi['<pad>']\n",
    "wtoi['<pad_fix>'] = old_pad\n",
    "wtoi['<pad>'] = '0'\n",
    "itow[old_pad] = '<pad_fix>'\n",
    "itow['0'] = '<pad>'\n",
    "\n",
    "# print wtoi['<pad>']\n",
    "# print itow['0']\n",
    "# print wtoi['<pad_fix>']\n",
    "\n",
    "assert(wtoi['<pad>'] == '0')\n",
    "assert(itow['0'] == '<pad>')\n",
    "\n",
    "\n",
    "# print val_data_json.keys()\n",
    "# print val_data_h5.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851\n"
     ]
    }
   ],
   "source": [
    "itoa = val_data_json['ix_to_ans']\n",
    "print(len(itoa))\n",
    "# print len(itoa)\n",
    "# print itoa['38']\n",
    "# print itoa\n",
    "unique_img_val = val_data_json['unique_img_val']\n",
    "ques_val = val_data_h5['ques_val'][:]\n",
    "ans_val = val_data_h5['ans_val'][:]\n",
    "question_id_val = val_data_h5['question_id_val'][:]\n",
    "img_pos_val = val_data_h5['img_pos_val'][:]\n",
    "images = np.array(imgs2batch(unique_img_val, img_pos_val, transform=transform))\n",
    "ques_val = np.array(ques_val)\n",
    "ans_val = np.array(ans_val).reshape((-1, 1))\n",
    "\n",
    "# print images.shape\n",
    "\n",
    "images = torch.from_numpy(images)\n",
    "ques_val = torch.from_numpy(ques_val)\n",
    "ans_val = torch.from_numpy(ans_val)\n",
    "\n",
    "# print images.size()\n",
    "# print ques_val.size()\n",
    "# print ans_val.size()\n",
    "# print(ans_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(ques_val.size(1)):\n",
    "#     print ' '.join(filter(lambda kx: kx!='<pad>',(map(lambda wr: itow[str(wr)], ques_val[i].detach().numpy().tolist()))))\n",
    "#     print ' '.join(map(lambda wr: itoa[str(wr)], ans_val[i].detach().numpy().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 3, 224, 224]) torch.Size([4000, 27])\n",
      "torch.Size([1000, 3, 224, 224]) torch.Size([1000, 27])\n"
     ]
    }
   ],
   "source": [
    "ques_ans_val = torch.cat((ques_val, ans_val), dim=1)\n",
    "BATCH_SIZE = 100\n",
    "split_point = int(0.2 * ques_ans_val.size(0)) # split 20% for testing\n",
    "\n",
    "ques_ans_splits = torch.split(ques_ans_val, split_point, dim=0)\n",
    "images_splits = torch.split(images, split_point, dim=0)\n",
    "\n",
    "ques_ans_test = ques_ans_splits[0]\n",
    "ques_ans_train = torch.cat(ques_ans_splits[1:], dim=0)\n",
    "\n",
    "images_test = images_splits[0]\n",
    "images_train = torch.cat(images_splits[1:], dim=0)\n",
    "\n",
    "# should be (torch.Size([TRAIN_SIZE, 3, 224, 224]), torch.Size([TRAIN_SIZE, MAX_LENGTH]))\n",
    "print(images_train.size(), ques_ans_train.size()) \n",
    "# should be (torch.Size([TEST_SIZEZ, 3, 224, 224]), torch.Size([TEST_SIZE, MAX_LENGTH]))\n",
    "print(images_test.size(), ques_ans_test.size())\n",
    "\n",
    "train_dataset=Data.TensorDataset(images_train, ques_ans_train)\n",
    "test_dataset=Data.TensorDataset(images_test, ques_ans_test)\n",
    "train_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "test_loader = Data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from naive_lstm import Enc, Dec\n",
    "# from naive_rnn import Enc, Dec\n",
    "# from naive_gru import Enc, Dec\n",
    "# from fusion_lstm import Enc, Dec\n",
    "from att_model_2 import Enc, Dec\n",
    "\n",
    "device = torch.device('cuda')\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "ques_vocab_size = len(itow)\n",
    "ans_vocab_size = len(itoa)+1\n",
    "num_layers = 1\n",
    "\n",
    "# print 'embed',embed_size,'hidden',hidden_size,'ques_vocab',ques_vocab_size, 'ans_vocab',ans_vocab_size\n",
    "encoder = Enc(embed_size).to(device)\n",
    "decoder = Dec(embed_size, hidden_size, ques_vocab_size, ans_vocab_size, num_layers).cuda()\n",
    "# encoder.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'lengths'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2ef91391652b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#         print '[OUT] features',features.size()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#         print '[OUT] output ', output.size()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/vqa/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'lengths'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# optimizer and loss\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(params,lr=lr)\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "# start your train\n",
    "lossList = []\n",
    "accList = []\n",
    "for epoch in range(30):\n",
    "    for i, (images, img_ans_val) in enumerate(train_loader):\n",
    "        ques, ans = torch.split(img_ans_val, 26,dim=1)\n",
    "        # images of shape [batch, 3, 256, 256]\n",
    "        # ques of shape [batch, 26]\n",
    "        # ans of shape [batch, 1]\n",
    "        \n",
    "#         print(images.shape)\n",
    "\n",
    "        lengths = []\n",
    "        for qix in ques:\n",
    "            for iy in range(len(qix)):\n",
    "                if (qix[iy]==0):\n",
    "                    lengths.append(iy)\n",
    "                    break;\n",
    "        tups = []\n",
    "        for ix in range(ques.size(0)):\n",
    "            row = ques[ix,:]\n",
    "            length = lengths[ix]\n",
    "            image_i = images[ix,:]\n",
    "            ans_i = ans[ix,:]\n",
    "            tup = (row, length, image_i, ans_i)\n",
    "            tups.append(tup)\n",
    "\n",
    "        sorted_tuples = sorted(tups, key=lambda tup: tup[1], reverse=True)\n",
    "        questions = torch.stack(list(map(lambda tup: tup[0], sorted_tuples)))\n",
    "        images = torch.stack(list(map(lambda tup: tup[2], sorted_tuples)))\n",
    "        answers = torch.stack(list(map(lambda tup: tup[3], sorted_tuples)))\n",
    "        lengths = list(map(lambda tup: tup[1], sorted_tuples))\n",
    "        \n",
    "#         print 'images', images.size()\n",
    "#         print 'img_ans_val', img_ans_val.size()\n",
    "#         print 'questions', questions.size()\n",
    "#         print 'answers', answers.size()\n",
    "#         print 'lengths', lengths\n",
    "    \n",
    "        images = images.to(device)\n",
    "        questions = questions.to(device).long()\n",
    "        \n",
    "        \n",
    "        raw_features, features = encoder(images)\n",
    "        raw_features = raw_features.cuda()\n",
    "                \n",
    "#         print '[OUT] features',features.size()\n",
    "        output = decoder(features, questions, lengths).cuda()\n",
    "        answers = answers.reshape((-1)).long().cuda()\n",
    "#         print '[OUT] output ', output.size()\n",
    "#         print '[OUT] answers ', answers.size()\n",
    "        loss = F.nll_loss(output, answers)\n",
    "\n",
    "        # copy here\n",
    "        lossList.append(loss.item())\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        \n",
    "        correct = pred.eq(answers.long().view_as(pred)).sum()\n",
    "        acc = float(correct) / float(BATCH_SIZE)\n",
    "        \n",
    "        accList.append(acc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 50 == 0: \n",
    "            print('epoch',epoch,'#', i, 'loss:', loss.item(), 'acc:', acc, 'correct:', correct)\n",
    "        \n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(' ')\n",
    "print(' TEST ')\n",
    "print(' ')\n",
    "\n",
    "lossList_test = []\n",
    "accList_test = []\n",
    "\n",
    "for i, (images, img_ans_val) in enumerate(test_loader):\n",
    "        ques, ans = torch.split(img_ans_val, 26,dim=1)\n",
    "        # images of shape [batch, 3, 256, 256]\n",
    "        # ques of shape [batch, 26]\n",
    "        # ans of shape [batch, 1]\n",
    "\n",
    "        lengths = []\n",
    "        for qix in ques:\n",
    "            for iy in range(len(qix)):\n",
    "                if (qix[iy]==0):\n",
    "                    lengths.append(iy)\n",
    "                    break;\n",
    "        tups = []\n",
    "        for ix in range(ques.size(0)):\n",
    "            row = ques[ix,:]\n",
    "            length = lengths[ix]\n",
    "            image_i = images[ix,:]\n",
    "            ans_i = ans[ix,:]\n",
    "            tup = (row, length, image_i, ans_i)\n",
    "            tups.append(tup)\n",
    "\n",
    "        sorted_tuples = sorted(tups, key=lambda tup: tup[1], reverse=True)\n",
    "        questions = torch.stack(list(map(lambda tup: tup[0], sorted_tuples)))\n",
    "        images = torch.stack(list(map(lambda tup: tup[2], sorted_tuples)))\n",
    "        answers = torch.stack(list(map(lambda tup: tup[3], sorted_tuples)))\n",
    "        lengths = list(map(lambda tup: tup[1], sorted_tuples))\n",
    "        \n",
    "#         print 'images', images.size()\n",
    "#         print 'img_ans_val', img_ans_val.size()\n",
    "#         print 'questions', questions.size()\n",
    "#         print 'answers', answers.size()\n",
    "#         print 'lengths', lengths\n",
    "    \n",
    "        images = images.to(device)\n",
    "        questions = questions.to(device).long()\n",
    "        features = encoder(images).cuda()\n",
    "#         print '[OUT] features',features.size()\n",
    "        output = decoder(features, questions, lengths).cuda()\n",
    "        answers = answers.reshape((-1)).long().cuda()\n",
    "#         print '[OUT] output ', output.size()\n",
    "#         print '[OUT] answers ', answers.size()\n",
    "        loss = F.nll_loss(output, answers)\n",
    "\n",
    "        # copy here\n",
    "        lossList_test.append(loss.item())\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        \n",
    "        correct = pred.eq(answers.long().view_as(pred)).sum()\n",
    "        acc = float(correct) / float(BATCH_SIZE)\n",
    "        \n",
    "        accList_test.append(acc)\n",
    "        \n",
    "        if i % 5 == 0: \n",
    "            print('epoch',epoch,'#', i, 'loss:', loss.item(), 'acc:', acc, 'correct:', correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ATTENTION'\n",
    "train_param = str(lr) + '_' + str(embed_size) + '_' + str(hidden_size)\n",
    "fname = model_type + '_' + train_param\n",
    "f_path = 'result/' + fname + '.pkl'\n",
    "\n",
    "print(f_path)\n",
    "\n",
    "with open(f_path, 'wb') as f:\n",
    "    pickle.dump([lossList, accList, lossList_test, accList_test], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(lossList)), lossList, 'ro')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(accList)), accList, 'ro')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(lossList_test)), lossList_test, 'bo')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(accList_test)), accList_test, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
